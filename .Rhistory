<<<<<<< HEAD
Predicted_Prob = mean(.pred_Enrolled)
)
set.seed(221102)
assessment_test |>
count(Status) |>
mutate(prop = n / sum(n))
# CV folds
cv_folds <-
analysis_train |>
vfold_cv(v = 10, strata = Status)
# Defining the random forest model
rf_recipe_downsample <-
recipe(Status ~., data = analysis_train) |>
step_rm(AppDate, OfferDate, ResponseDate) |>
update_role(AppDate, OfferDate, ResponseDate, new_role = "metadata") |>
step_dummy(all_nominal_predictors()) |>
step_zv(all_predictors()) |>
step_normalize(all_predictors()) |>
themis::step_downsample(Status)
rf_recipe_downsample
# Define the random forest model
rf_model_tune <-
rand_forest(mtry = tune(), trees = 1000) |>
set_mode("classification") |>
set_engine("ranger", importance = "permutation")
rf_tune_wf <-
workflow() |>
add_recipe(rf_recipe_downsample) |>
add_model(rf_model_tune)
class_metrics <- metric_set(
accuracy, sensitivity, specificity, precision,
kap, roc_auc)
# Tuning the model
rf_tune_grid <- grid_regular(mtry(range = c(1, 14)), levels = 14)
rf_tune_grid
num_cores <- parallel::detectCores()
num_cores
doParallel::registerDoParallel(cores = num_cores - 1L)
set.seed(231164)
rf_tune_res <- tune_grid(
rf_tune_wf,
resamples = cv_folds,
grid = rf_tune_grid,
metrics = class_metrics
)
beepr::beep()
rf_tune_res |>
collect_metrics() |>
filter(.metric %in% c("sensitivity", "specificity")) |>
ggplot(aes(
x = mtry, y = mean, ymin = mean - std_err,
ymax = mean + std_err,
colour = .metric
)) +
geom_errorbar() +
geom_line() +
geom_point() +
scale_colour_manual(values = c("#D55E00", "#0072B2")) +
facet_wrap(~.metric, ncol = 1, scales = "free_y") +
guides(colour = "none") +
theme_bw()
rf_tune_res |>
collect_metrics() |>
filter(.metric %in% c("sensitivity", "specificity")) |>
ggplot(aes(
x = mtry, y = mean, ymin = mean - std_err,
ymax = mean + std_err,
colour = .metric
)) +
geom_errorbar() +
geom_line() +
geom_point() +
scale_colour_manual(values = c("#D55E00", "#0072B2")) +
facet_wrap(~.metric, ncol = 1, scales = "free_y") +
guides(colour = "none") +
theme_bw()
rf_tune_res |>
collect_metrics() |>
filter(.metric %in% c("sensitivity", "specificity")) |>
ggplot(aes(
x = mtry, y = mean, ymin = mean - std_err,
ymax = mean + std_err,
colour = .metric
)) +
geom_errorbar() +
geom_line() +
geom_point() +
scale_colour_manual(values = c("#D55E00", "#0072B2")) +
facet_wrap(~.metric, ncol = 1, scales = "free_y") +
guides(colour = "none") +
theme_bw()
rf_tune_res |>
collect_metrics() |>
filter(.metric %in% c("sensitivity", "specificity")) |>
ggplot(aes(
x = mtry, y = mean, ymin = mean - std_err,
ymax = mean + std_err,
colour = .metric
)) +
geom_errorbar() +
geom_line() +
geom_point() +
scale_colour_manual(values = c("#D55E00", "#0072B2")) +
facet_wrap(~.metric, ncol = 1, scales = "free_y") +
guides(colour = "none") +
theme_bw()
rf_tune_res |>
collect_metrics() |>
filter(.metric %in% c("roc_auc", "accuracy", "kap")) |>
ggplot(aes(
x = mtry, y = mean, ymin = mean - std_err,
ymax = mean + std_err, colour = .metric
)) +
geom_errorbar() +
geom_line() +
geom_point() +
scale_colour_manual(values = c("#D55E00", "#0072B2", "#009E73")) +
facet_wrap(~.metric, ncol = 1, scales = "free_y") +
guides(colour = "none") +
theme_bw()
best_rf <- select_best(rf_tune_res, "sensitivity")
rf_final_wf <- finalize_workflow(rf_tune_wf, best_rf)
rf_final_wf
# Test set
set.seed(666420)
rf_final_fit <-
rf_final_wf |>
last_fit(analysis_assessment_split, metrics = class_metrics)
rf_final_fit |>
collect_metrics()
rf_final_fit |>
collect_predictions() |>
conf_mat(truth = Status, estimate = .pred_class)
# Working with final data set
rf_final_model <- rf_final_wf %>%
last_fit(final_training_prediction_split)
beepr::beep()
rf_final_model |>
augment() |>
group_by(Program) |>
summarise(
Predicted_N = sum(.pred_Enrolled >= .4),
Predicted_Prob = mean(.pred_Enrolled)
)
rf_final_model |>
augment() |>
group_by(Program) |>
summarise(
Predicted_N = sum(.pred_Enrolled >= .3),
Predicted_Prob = mean(.pred_Enrolled)
)
rf_final_model |>
augment() |>
group_by(Program) |>
summarise(
Predicted_N = sum(.pred_Enrolled >= .45),
Predicted_Prob = mean(.pred_Enrolled)
)
rf_final_model |>
augment() |>
group_by(Program) |>
summarise(
Predicted_N = sum(.pred_Enrolled >= .7),
Predicted_Prob = mean(.pred_Enrolled)
)
rf_final_model |>
augment() |>
group_by(Program) |>
summarise(
Predicted_N = sum(.pred_Enrolled >= .3),
Predicted_Prob = mean(.pred_Enrolled)
)
=======
cv_folds <- vfold_cv(analysis_train, v = 7) #I have no clue if it's needed
grid_lasso <- grid_regular(penalty(),
levels = 20)
lasso_tune <-
lasso_wf %>%
tune_grid(resamples = cv_folds,
grid = grid_lasso,
metrics = metric_set(rmse, rsq_trad, mae))
beepr::beep()
analysis_train
linear_reg_recipe <-
recipe(Status ~ ., data = analysis_train) |>
step_rm(AppDate, OfferDate,ResponseDate, Program)|>
step_dummy(all_nominal_predictors()) |>
#step_dummy(all_outcomes()) |>
# I still include interaction terms
#step_interact(~ all_predictors():all_predictors()) |>
# remove any resulting variables that have only one value
# and thus zero variance ("zv")
step_zv(all_predictors()) |>
# normalize the predictors to have mean 0 and SD 1
step_normalize(all_predictors())
lasso_wf <-
workflow() |>
add_recipe(linear_reg_recipe) |>
add_model(lasso_reg)
set.seed(1810)
cv_folds <- vfold_cv(analysis_train, v = 7) #I have no clue if it's needed
grid_lasso <- grid_regular(penalty(),
levels = 20)
lasso_tune <-
lasso_wf %>%
tune_grid(resamples = cv_folds,
grid = grid_lasso,
metrics = metric_set(rmse, rsq_trad, mae))
beepr::beep()
library(tidymodels)
library(tidyverse)
library(beepr)
source("./data_processing.R")
lasso_reg <- linear_reg(penalty = tune(), mixture = 1) %>%
set_engine("glmnet")
linear_reg_recipe <-
recipe(Status ~ ., data = analysis_train) |>
step_rm(AppDate, OfferDate,ResponseDate)|>
step_dummy(all_nominal_predictors()) |>
step_dummy(all_outcomes()) |>
# I still include interaction terms
#step_interact(~ all_predictors():all_predictors()) |>
# remove any resulting variables that have only one value
# and thus zero variance ("zv")
step_zv(all_predictors()) |>
# normalize the predictors to have mean 0 and SD 1
step_normalize(all_predictors())
lasso_wf <-
workflow() |>
add_recipe(linear_reg_recipe) |>
add_model(lasso_reg)
set.seed(1810)
cv_folds <- vfold_cv(analysis_train, v = 7) #I have no clue if it's needed
grid_lasso <- grid_regular(penalty(c(1, 4), trans = log10_trans()),
levels = 40)
lasso_tune <-
lasso_wf %>%
tune_grid(resamples = cv_folds,
grid = grid_lasso,
metrics = metric_set(rmse, rsq_trad, mae))
beepr::beep()
show_notes(.Last.tune.result)
analysis_train
library(tidymodels)
library(tidyverse)
library(beepr)
source("./data_processing.R")
lasso_logistic_reg <-
logistic_reg(penalty = tune(), mixture = 1) |>
set_engine("glmnet")
lasso_recipe <-
recipe(Status ~ ., data = analysis_train) |>
step_rm(AppDate, OfferDate,ResponseDate)|>
step_dummy(all_nominal_predictors()) |>
step_dummy(all_outcomes()) |>
# I still include interaction terms
#step_interact(~ all_predictors():all_predictors()) |>
# remove any resulting variables that have only one value
# and thus zero variance ("zv")
step_zv(all_predictors()) |>
# normalize the predictors to have mean 0 and SD 1
step_normalize(all_predictors())
lasso_wf <-
workflow() |>
add_recipe(lasso_recipe) |>
add_model(lasso_logistic_reg)
set.seed(1810)
cv_folds <- vfold_cv(analysis_train, v = 10)
grid_lasso <- grid_regular(penalty(c(1, 4), trans = log10_trans()),
levels = 40)
lasso_tune <-
lasso_wf %>%
tune_grid(resamples = cv_folds,
grid = grid_lasso,
metrics = metric_set(rmse, rsq_trad, mae))
grid_lasso <-
grid_regular(penalty(range = c(-4.5, -.5),
trans = log10_trans()),
levels = 100)
lasso_tune <-
lasso_wf |>
tune_grid(resamples = cv_folds,
grid = grid_lasso,
metrics = metric_set(accuracy, f_meas, kap, bal_accuracy))
beepr::beep()
show_notes(.Last.tune.result)
library(tidymodels)
library(tidyverse)
library(beepr)
source("./data_processing.R")
lasso_logistic_reg <-
logistic_reg(penalty = tune(), mixture = 1) |>
set_engine("glmnet")
lasso_recipe <-
recipe(Status ~ ., data = analysis_train) |>
step_rm(AppDate, OfferDate,ResponseDate)|>
step_dummy(all_nominal_predictors()) |>
step_dummy(all_outcomes()) |>
# I still include interaction terms
#step_interact(~ all_predictors():all_predictors()) |>
# remove any resulting variables that have only one value
# and thus zero variance ("zv")
step_zv(all_predictors()) |>
# normalize the predictors to have mean 0 and SD 1
step_normalize(all_predictors())
lasso_recipe <-
recipe(Status ~ ., data = analysis_train) |>
step_rm(AppDate, OfferDate,ResponseDate)|>
step_dummy(all_nominal_predictors()) |>
# step_dummy(all_outcomes()) |>
# I still include interaction terms
#step_interact(~ all_predictors():all_predictors()) |>
# remove any resulting variables that have only one value
# and thus zero variance ("zv")
step_zv(all_predictors()) |>
# normalize the predictors to have mean 0 and SD 1
step_normalize(all_predictors())
lasso_wf <-
workflow() |>
add_recipe(lasso_recipe) |>
add_model(lasso_logistic_reg)
set.seed(1810)
cv_folds <- vfold_cv(analysis_train, v = 10) #I have no clue if it's needed
lasso_tune <-
lasso_wf |>
tune_grid(resamples = cv_folds,
grid = grid_lasso,
metrics = metric_set(accuracy, f_meas, kap, bal_accuracy))
grid_lasso <-
grid_regular(penalty(range = c(-4.5, -.5),
trans = log10_trans()),
levels = 100)
lasso_tune <-
lasso_wf |>
tune_grid(resamples = cv_folds,
grid = grid_lasso,
metrics = metric_set(accuracy, f_meas, kap, bal_accuracy))
lasso_tune_metrics <-
lasso_tune %>%
collect_metrics()
lasso_tune_metrics |>
filter(.metric == "rmse") |>
ggplot(aes(x = penalty, y = mean,
ymin = mean - std_err, ymax = mean + std_err)) +
geom_pointrange(alpha = 0.5) +
scale_x_log10() +
labs(y = "RMSE", x = expression(lambda)) +
theme_bw()
lasso_tune_metrics
lasso_tune_metrics |>
filter(.metric == "accuracy") |>
ggplot(aes(x = penalty, y = mean,
ymin = mean - std_err, ymax = mean + std_err)) +
geom_pointrange(alpha = 0.5, size = .125) +
scale_x_log10() +
labs(y = "Accuracy", x = expression(lambda)) +
theme_bw()
grid_lasso <-
grid_regular(penalty(range = c(-4.5, -2),
trans = log10_trans()),
levels = 100)
lasso_tune <-
lasso_wf |>
tune_grid(resamples = cv_folds,
grid = grid_lasso,
metrics = metric_set(accuracy, f_meas, kap, bal_accuracy))
beepr::beep()
lasso_tune_metrics <-
lasso_tune |>
collect_metrics()
lasso_tune_metrics |>
filter(.metric == "accuracy") |>
ggplot(aes(x = penalty, y = mean,
ymin = mean - std_err, ymax = mean + std_err)) +
geom_pointrange(alpha = 0.5, size = .125) +
scale_x_log10() +
labs(y = "Accuracy", x = expression(lambda)) +
theme_bw()
grid_lasso <-
grid_regular(penalty(range = c(-4.5, -1.5),
trans = log10_trans()),
levels = 100)
lasso_tune <-
lasso_wf |>
tune_grid(resamples = cv_folds,
grid = grid_lasso,
metrics = metric_set(accuracy, f_meas, kap, bal_accuracy))
#beepr::beep()
lasso_tune_metrics <-
lasso_tune |>
collect_metrics()
lasso_tune_metrics |>
filter(.metric == "accuracy") |>
ggplot(aes(x = penalty, y = mean,
ymin = mean - std_err, ymax = mean + std_err)) +
geom_pointrange(alpha = 0.5, size = .125) +
scale_x_log10() +
labs(y = "Accuracy", x = expression(lambda)) +
theme_bw()
grid_lasso <-
grid_regular(penalty(range = c(-4, -1.5),
trans = log10_trans()),
levels = 100)
lasso_tune <-
lasso_wf |>
tune_grid(resamples = cv_folds,
grid = grid_lasso,
metrics = metric_set(accuracy, f_meas, kap, bal_accuracy))
#beepr::beep()
lasso_tune_metrics <-
lasso_tune |>
collect_metrics()
lasso_tune_metrics |>
filter(.metric == "accuracy") |>
ggplot(aes(x = penalty, y = mean,
ymin = mean - std_err, ymax = mean + std_err)) +
geom_pointrange(alpha = 0.5, size = .125) +
scale_x_log10() +
labs(y = "Accuracy", x = expression(lambda)) +
theme_bw()
grid_lasso <-
grid_regular(penalty(range = c(-4, -1.3),
trans = log10_trans()),
levels = 100)
lasso_tune <-
lasso_wf |>
tune_grid(resamples = cv_folds,
grid = grid_lasso,
metrics = metric_set(accuracy, f_meas, kap, bal_accuracy))
#beepr::beep()
lasso_tune_metrics <-
lasso_tune |>
collect_metrics()
lasso_tune_metrics |>
filter(.metric == "accuracy") |>
ggplot(aes(x = penalty, y = mean,
ymin = mean - std_err, ymax = mean + std_err)) +
geom_pointrange(alpha = 0.5, size = .125) +
scale_x_log10() +
labs(y = "Accuracy", x = expression(lambda)) +
theme_bw()
grid_lasso <-
grid_regular(penalty(range = c(-4, -1.1),
trans = log10_trans()),
levels = 100)
lasso_tune <-
lasso_wf |>
tune_grid(resamples = cv_folds,
grid = grid_lasso,
metrics = metric_set(accuracy, f_meas, kap, bal_accuracy))
#beepr::beep()
lasso_tune_metrics <-
lasso_tune |>
collect_metrics()
lasso_tune_metrics |>
filter(.metric == "accuracy") |>
ggplot(aes(x = penalty, y = mean,
ymin = mean - std_err, ymax = mean + std_err)) +
geom_pointrange(alpha = 0.5, size = .125) +
scale_x_log10() +
labs(y = "Accuracy", x = expression(lambda)) +
theme_bw()
grid_lasso <-
grid_regular(penalty(range = c(-4.5, -1.5),
trans = log10_trans()),
levels = 100)
lasso_tune <-
lasso_wf |>
tune_grid(resamples = cv_folds,
grid = grid_lasso,
metrics = metric_set(accuracy, f_meas, kap, bal_accuracy))
#beepr::beep()
lasso_tune_metrics <-
lasso_tune |>
collect_metrics()
lasso_tune_metrics |>
filter(.metric == "accuracy") |>
ggplot(aes(x = penalty, y = mean,
ymin = mean - std_err, ymax = mean + std_err)) +
geom_pointrange(alpha = 0.5, size = .125) +
scale_x_log10() +
labs(y = "Accuracy", x = expression(lambda)) +
theme_bw()
lasso_1se_model <-
lasso_tune |>
select_by_one_std_err(metric = "accuracy", desc(penalty))
lasso_1se_model
lasso_wf_tuned <-
lasso_wf |>
finalize_workflow(lasso_1se_model)
lasso_wf_tuned
lasso_recipe <-
recipe(Status ~ ., data = analysis_train) |>
step_rm(AppDate, OfferDate,ResponseDate)|>
step_dummy(all_nominal_predictors()) |>
step_dummy(all_outcomes()) |>
# I still include interaction terms
#step_interact(~ all_predictors():all_predictors()) |>
# remove any resulting variables that have only one value
# and thus zero variance ("zv")
step_zv(all_predictors()) |>
# normalize the predictors to have mean 0 and SD 1
step_normalize(all_predictors())
lasso_wf <-
workflow() |>
add_recipe(lasso_recipe) |>
add_model(lasso_logistic_reg)
set.seed(1810)
cv_folds <- vfold_cv(analysis_train, v = 10) #I have no clue if it's needed
grid_lasso <-
grid_regular(penalty(range = c(-4.5, -1.5),
trans = log10_trans()),
levels = 100)
lasso_tune <-
lasso_wf |>
tune_grid(resamples = cv_folds,
grid = grid_lasso,
metrics = metric_set(accuracy, f_meas, kap, bal_accuracy))
lasso_1se_model
lasso_recipe <-
recipe(Status ~ ., data = analysis_train) |>
step_rm(AppDate, OfferDate,ResponseDate)|>
step_dummy(all_nominal_predictors()) |>
# I still include interaction terms
step_interact(~ all_predictors():all_predictors()) |>
# remove any resulting variables that have only one value
# and thus zero variance ("zv")
step_zv(all_predictors()) |>
# normalize the predictors to have mean 0 and SD 1
step_normalize(all_predictors())
lasso_wf <-
workflow() |>
add_recipe(lasso_recipe) |>
add_model(lasso_logistic_reg)
set.seed(1810)
cv_folds <- vfold_cv(analysis_train, v = 10) #I have no clue if it's needed
grid_lasso <-
grid_regular(penalty(range = c(-4.5, -1.5),
trans = log10_trans()),
levels = 100)
lasso_tune <-
lasso_wf |>
tune_grid(resamples = cv_folds,
grid = grid_lasso,
metrics = metric_set(accuracy, f_meas, kap, bal_accuracy))
#Well, at least it works now. But we`ll need to discuss which metrics to take.
beepr::beep()
lasso_tune_metrics <-
lasso_tune |>
collect_metrics()
lasso_tune_metrics |>
filter(.metric == "accuracy") |>
ggplot(aes(x = penalty, y = mean,
ymin = mean - std_err, ymax = mean + std_err)) +
geom_pointrange(alpha = 0.5, size = .125) +
scale_x_log10() +
labs(y = "Accuracy", x = expression(lambda)) +
theme_bw()
lasso_tune |>
select_by_one_std_err(metric = "accuracy", desc(penalty))
grid_lasso <-
grid_regular(penalty(range = c(-4.5, -.5),
trans = log10_trans()),
levels = 100)
lasso_tune <-
lasso_wf |>
tune_grid(resamples = cv_folds,
grid = grid_lasso,
metrics = metric_set(accuracy, f_meas, kap, bal_accuracy))
#Well, at least it works now. But we`ll need to discuss which metrics to take.
beepr::beep()
lasso_tune_metrics <-
lasso_tune |>
collect_metrics()
lasso_tune_metrics |>
filter(.metric == "accuracy") |>
ggplot(aes(x = penalty, y = mean,
ymin = mean - std_err, ymax = mean + std_err)) +
geom_pointrange(alpha = 0.5, size = .125) +
scale_x_log10() +
labs(y = "Accuracy", x = expression(lambda)) +
theme_bw()
lasso_tune |>
select_by_one_std_err(metric = "accuracy", desc(penalty))
library(tidymodels)
library(tidyverse)
library(beepr)
source("./data_processing.R")
lasso_logistic_reg <-
logistic_reg(penalty = tune(), mixture = 1) |>
set_engine("glmnet")
>>>>>>> main
lasso_recipe <-
recipe(Status ~ ., data = analysis_train) |>
step_rm(AppDate, OfferDate,ResponseDate)|>
step_dummy(all_nominal_predictors()) |>
# I still include interaction terms
#step_interact(~ all_predictors():all_predictors()) |>
# remove any resulting variables that have only one value
# and thus zero variance ("zv")
step_zv(all_predictors()) |>
# normalize the predictors to have mean 0 and SD 1
<<<<<<< HEAD
step_normalize(all_predictors()) |>
themis::step_downsample(Status)
=======
step_normalize(all_predictors())
>>>>>>> main
lasso_wf <-
workflow() |>
add_recipe(lasso_recipe) |>
add_model(lasso_logistic_reg)
set.seed(1810)
<<<<<<< HEAD
cv_folds <- vfold_cv(analysis_train, v = 10, strata = Status) #I have no clue if it's needed
=======
cv_folds <- vfold_cv(analysis_train, v = 10) #I have no clue if it's needed
grid_lasso <-
grid_regular(penalty(range = c(-4.5, -.5),
trans = log10_trans()),
levels = 100)
lasso_tune <-
lasso_wf |>
tune_grid(resamples = cv_folds,
grid = grid_lasso,
metrics = metric_set(accuracy, f_meas, kap, bal_accuracy))
#Well, at least it works now. But we`ll need to discuss which metrics to take.
beepr::beep()
lasso_tune_metrics <-
lasso_tune |>
collect_metrics()
lasso_tune_metrics |>
filter(.metric == "accuracy") |>
ggplot(aes(x = penalty, y = mean,
ymin = mean - std_err, ymax = mean + std_err)) +
geom_pointrange(alpha = 0.5, size = .125) +
scale_x_log10() +
labs(y = "Accuracy", x = expression(lambda)) +
theme_bw()
# Assume that the correct model is already chosen
lasso_1se_model <-
lasso_tune |>
select_by_one_std_err(metric = "accuracy", desc(penalty))
acc <- 0.956
lasso_wf_tuned <-
lasso_wf |>
finalize_workflow(lasso_1se_model)
>>>>>>> main
grid_lasso <-
grid_regular(penalty(range = c(-4.5, -1.5),
trans = log10_trans()),
levels = 100)
lasso_tune <-
lasso_wf |>
tune_grid(resamples = cv_folds,
grid = grid_lasso,
<<<<<<< HEAD
metrics = metric_set(accuracy, f_meas, kap, bal_accuracy, sensitivity, precision))
=======
metrics = metric_set(accuracy, f_meas, kap, bal_accuracy))
>>>>>>> main
#Well, at least it works now. But we`ll need to discuss which metrics to take.
beepr::beep()
lasso_tune_metrics <-
lasso_tune |>
collect_metrics()
lasso_tune_metrics |>
filter(.metric == "accuracy") |>
ggplot(aes(x = penalty, y = mean,
ymin = mean - std_err, ymax = mean + std_err)) +
geom_pointrange(alpha = 0.5, size = .125) +
scale_x_log10() +
labs(y = "Accuracy", x = expression(lambda)) +
theme_bw()
# Assume that the correct model is already chosen
lasso_1se_model <-
lasso_tune |>
<<<<<<< HEAD
select_by_one_std_err(metric = "sensitivity", desc(penalty))
lasso_wf_tuned <-
lasso_wf |>
finalize_workflow(lasso_1se_model)
# I guess last fit?
lasso_last_fit <-
lasso_wf_tuned |>
last_fit(analysis_assessment_split, metrics = metric_set(accuracy, f_meas, kap, bal_accuracy, sensitivity, precision))
lasso_test_metrics <- lasso_last_fit |>
collect_metrics()
lasso_test_metrics
=======
select_by_one_std_err(metric = "accuracy", desc(penalty))
acc <- 0.956
lasso_wf_tuned <-
lasso_wf |>
finalize_workflow(lasso_1se_model)
lasso_last_fit <-
lasso_wf_tuned |>
last_fit(analysis_assessment_split, metrics = metric_set(accuracy, f_meas, kap, bal_accuracy))
lasso_last_fit
lasso_test_metrics <-
lasso_test_metrics |>
select(.metric, .estimate) |>
mutate(model = "lasso")
lasso_test_metrics <-
lasso_last_fit |>
collect_metrics()
>>>>>>> main
lasso_test_metrics <-
lasso_test_metrics |>
select(.metric, .estimate) |>
mutate(model = "lasso")
<<<<<<< HEAD
lasso_final_model <- lasso_wf_tuned %>%
last_fit(final_training_prediction_split)
lasso_final_model |>
augment() |>
group_by(Program) |>
summarise(
Predicted_N = sum(.pred_Enrolled >= 0.3),
Predicted_Prob = mean(.pred_Enrolled)
)
lasso_final_model |>
augment() |>
group_by(Program) |>
summarise(
Predicted_N = sum(.pred_Enrolled >= 0.3),
Predicted_Prob = mean(.pred_Enrolled)
)
lasso_final_model |>
augment() |>
group_by(Program) |>
summarise(
Predicted_N = sum(.pred_Enrolled >= 0.4),
Predicted_Prob = mean(.pred_Enrolled)
)
lasso_final_model |>
augment() |>
group_by(Program) |>
summarise(
Predicted_N = sum(.pred_Enrolled >= 0.3),
Predicted_Prob = mean(.pred_Enrolled)
)
lasso_final_model |>
augment() |>
group_by(Program) |>
summarise(
Predicted_N = sum(.pred_Enrolled >= 0.2),
Predicted_Prob = mean(.pred_Enrolled)
)
lasso_final_model |>
augment() |>
group_by(Program) |>
summarise(
Predicted_N = sum(.pred_Enrolled >= 0.3),
Predicted_Prob = mean(.pred_Enrolled)
)
lasso_last_fit$.predictions[[1]] %>% conf_mat(truth = Status, estimate = .pred_class)
conf_mat_lasso[1]$table %>% sensitivity()
lasso_last_fit$.predictions[[1]] %>% conf_mat(truth = Status, estimate = .pred_class)
conf_mat_lasso[1]$table %>% sensitivity()
lasso_test_metrics
rf_tune_res |>
collect_metrics() |>
filter(.metric %in% c("sensitivity", "specificity")) |>
ggplot(aes(
x = mtry, y = mean, ymin = mean - std_err,
ymax = mean + std_err,
colour = .metric
)) +
geom_errorbar() +
geom_line() +
geom_point() +
scale_colour_manual(values = c("#D55E00", "#0072B2")) +
facet_wrap(~.metric, ncol = 1, scales = "free_y") +
guides(colour = "none") +
theme_bw()
rf_tune_res |>
collect_metrics() |>
filter(.metric %in% c("roc_auc", "accuracy", "kap")) |>
ggplot(aes(
x = mtry, y = mean, ymin = mean - std_err,
ymax = mean + std_err, colour = .metric
)) +
geom_errorbar() +
geom_line() +
geom_point() +
scale_colour_manual(values = c("#D55E00", "#0072B2", "#009E73")) +
facet_wrap(~.metric, ncol = 1, scales = "free_y") +
guides(colour = "none") +
theme_bw()
best_rf <- select_best(rf_tune_res, "sensitivity")
rf_final_wf <- finalize_workflow(rf_tune_wf, best_rf)
rf_final_wf
# Test set
set.seed(666420)
rf_final_fit <-
rf_final_wf |>
last_fit(analysis_assessment_split, metrics = class_metrics)
rf_final_fit |>
collect_metrics()
rf_final_fit |>
collect_predictions()
rf_final_fit |>
collect_predictions() |>
roc_curve(Status, .pred_Enrolled) |>
autoplot()
last_fit(analysis_assessment_split, metrics = metric_set(accuracy, f_meas, kap, bal_accuracy, sensitivity, precision, roc_auc)
lasso_test_metrics <- lasso_last_fit |>
# I guess last fit?
lasso_last_fit <-
lasso_wf_tuned |>
last_fit(analysis_assessment_split, metrics = metric_set(accuracy, f_meas, kap, bal_accuracy, sensitivity, precision, roc_auc))
lasso_test_metrics <- lasso_last_fit |>
collect_metrics()
lasso_test_metrics
lasso_last_fit |>
collect_predictions() |>
roc_curve(Status, .pred_Enrolled) |>
autoplot()
lasso_test_metrics
lasso_tune <-
lasso_wf |>
tune_grid(resamples = cv_folds,
grid = grid_lasso,
metrics = metric_set(accuracy, f_meas, kap, bal_accuracy, sensitivity, precision, specificity))
#Well, at least it works now. But we`ll need to discuss which metrics to take.
beepr::beep()
lasso_tune_metrics <-
lasso_tune |>
collect_metrics()
lasso_tune_metrics
lasso_tune_metrics |>
filter(.metric == "accuracy") |>
ggplot(aes(x = penalty, y = mean,
ymin = mean - std_err, ymax = mean + std_err)) +
geom_pointrange(alpha = 0.5, size = .125) +
scale_x_log10() +
labs(y = "Accuracy", x = expression(lambda)) +
theme_bw()
# Assume that the correct model is already chosen
lasso_1se_model <-
lasso_tune |>
select_by_one_std_err(metric = "sensitivity", desc(penalty))
lasso_wf_tuned <-
lasso_wf |>
finalize_workflow(lasso_1se_model)
# I guess last fit?
lasso_last_fit <-
lasso_wf_tuned |>
last_fit(analysis_assessment_split, metrics = metric_set(accuracy, f_meas, kap, bal_accuracy, sensitivity, precision, roc_auc))
lasso_test_metrics <- lasso_last_fit |>
collect_metrics()
lasso_test_metrics
# I guess last fit?
lasso_last_fit <-
lasso_wf_tuned |>
last_fit(analysis_assessment_split, metrics = metric_set(accuracy, f_meas, kap, bal_accuracy, sensitivity, precision, specificity))
lasso_test_metrics <- lasso_last_fit |>
collect_metrics()
lasso_test_metrics
View(offers)
assessment_test |>
count(Status) |>
mutate(prop = n / sum(n))
analysis_train |>
count(Status) |>
mutate(prop = n / sum(n))
final_training |>
count(Status) |>
mutate(prop = n / sum(n))
lasso_test_metrics
rf_final_fit |>
collect_metrics()
lasso_last_fit |>
extract_fit_parsnip() |>
tidy() |>
arrange(desc(abs(estimate)))
lasso_last_fit |>
collect_predictions() |>
roc_curve(Status, .pred_Enrolled) |>
autoplot()
lasso_tune_metrics <-
lasso_tune |>
collect_metrics()
lasso_tune <-
lasso_wf |>
tune_grid(resamples = cv_folds,
grid = grid_lasso,
metrics = metric_set(accuracy, f_meas, kap, bal_accuracy, sensitivity, precision, specificity))
#Well, at least it works now. But we`ll need to discuss which metrics to take.
beepr::beep()
lasso_tune_metrics <-
lasso_tune |>
collect_metrics()
lasso_tune_metrics |>
filter(.metric == "accuracy") |>
ggplot(aes(x = penalty, y = mean,
ymin = mean - std_err, ymax = mean + std_err)) +
geom_pointrange(alpha = 0.5, size = .125) +
scale_x_log10() +
labs(y = "Accuracy", x = expression(lambda)) +
theme_bw()
# Assume that the correct model is already chosen
lasso_1se_model <-
lasso_tune |>
select_by_one_std_err(metric = "sensitivity", desc(penalty))
lasso_wf_tuned <-
lasso_wf |>
finalize_workflow(lasso_1se_model)
# I guess last fit?
lasso_last_fit <-
lasso_wf_tuned |>
last_fit(analysis_assessment_split, metrics = metric_set(accuracy, f_meas, kap, bal_accuracy, sensitivity, precision, specificity))
lasso_test_metrics <- lasso_last_fit |>
collect_metrics()
lasso_test_metrics
lasso_last_fit |>
collect_predictions() |>
roc_curve(Status, .pred_Enrolled) |>
autoplot()
# I guess last fit?
lasso_last_fit <-
lasso_wf_tuned |>
last_fit(analysis_assessment_split, metrics = metric_set(accuracy, f_meas, kap, bal_accuracy, sensitivity, precision, specificity))
lasso_test_metrics <- lasso_last_fit |>
collect_metrics()
lasso_test_metrics
lasso_last_fit
lasso_last_fit |>
collect_predictions() |>
roc_curve(Status, .pred_Enrolled) |>
autoplot()
lasso_last_fit$.predictions[[1]] %>% conf_mat(truth = Status, estimate = .pred_class)
lasso_last_fit |>
collect_predictions() |>
roc_curve(Status, .pred_Enrolled) |>
autoplot()
lasso_last_fit |>
collect_predictions()
lasso_last_fit |>
collect_predictions() |>
roc_curve(Status, .pred_class) |>
autoplot()
lasso_final_model |>
augment() |>
group_by(Program) |>
summarise(
Predicted_N = sum(.pred_Enrolled >= 0.3),
Predicted_Prob = mean(.pred_Enrolled)
)
lasso_last_fit |>
collect_predictions() |>
roc_curve(Status, .pred_Enrolled) |>
autoplot()
library(tidymodels)
library(tidyverse)
library(ggplot2)
library(dplyr)
load("data/offers_censored.RData")
apps_year <- offers %>%
group_by(AppYear) %>%
summarise(num_enrolled = sum(Status == "Enrolled"),
num_notenroll = sum(Status == "Not enrolled")) %>%
filter(AppYear != 2023)
# Number of enrollment statuses in each year
ggplot(apps_year, aes(x = as.factor(AppYear))) +
geom_bar(aes(y = num_enrolled, fill = "Enrolled"), stat = "identity") +
geom_bar(aes(y = num_notenroll, fill = "Not Enrolled"), stat = "identity") +
labs(title = "Enrollment Status by Year",
x = "Year",
y = "Number of Offers",
fill = "Enrollment Status") +
scale_fill_manual(values = c("Enrolled" = "cornflowerblue", "Not Enrolled" = "pink3")) +
theme_minimal()
# Distribution of enrollment statuses
ggplot(offers, aes(x = Status)) +
geom_bar() +
labs(title = "Distribution of Enrollment Status",
x = "Status",
y = "Count")
# Number of applications per year
ggplot(offers, aes(x = as.factor(AppYear))) +
geom_bar() +
labs(title = "Distribution of Applications by Year",
x = "Application Year",
y = "Count")
offers %>%
filter(App4 == 3)
app4_3 <- offers %>%
filter(App4 == 3)
View(app4_3)
ggplot(data = offers)+
geom_bar(aes(x = App4, fill = Status), position = "fill")
View(offers)
=======
lasso_test_metrics
gc()
>>>>>>> main
